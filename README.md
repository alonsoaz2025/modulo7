# ğŸš€ Terraform Demo â€“ Pipeline de Datos con BigQuery y Looker

Este proyecto crea automÃ¡ticamente un **pipeline de datos completo** en Google Cloud, utilizando el dataset pÃºblico de Shakespeare para demostrar las fases de:

**Ingesta â†’ Procesamiento â†’ Almacenamiento â†’ VisualizaciÃ³n**

---

## ğŸ§© Componentes creados
| Fase | Recurso | DescripciÃ³n |
|------|----------|-------------|
| Ingesta | `google_storage_bucket` | Bucket base para pruebas |
| Procesamiento | `google_bigquery_table (view)` | Vista que agrega datos del dataset pÃºblico |
| Almacenamiento | `google_bigquery_dataset` | Dataset propio del alumno |
| VisualizaciÃ³n | Looker Studio | ConexiÃ³n directa a la vista analÃ­tica |

---

## ğŸ§° Requisitos
- Cuenta de servicio con permisos de BigQuery y Storage
- APIs habilitadas:
  - `bigquery.googleapis.com`
  - `storage.googleapis.com`
- Archivo `credentials.json` en la raÃ­z del proyecto

---

## â–¶ï¸ EjecuciÃ³n
```bash
# 1. Autenticarse con la cuenta de servicio
gcloud auth activate-service-account --key-file=credentials.json

# 2. Inicializar Terraform
cd terraform
terraform init

# 3. Planificar
terraform plan -var="project_id=<TU_PROJECT_ID>" -var="credentials_file=../credentials.json"

# 4. Desplegar
terraform apply -auto-approve -var="project_id=<TU_PROJECT_ID>" -var="credentials_file=../credentials.json"

# 5. Conectar a Looker Studio
# Ver instrucciones al final del despliegue
